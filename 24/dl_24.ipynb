{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM: Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_utils import *\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 21067.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10'),\n",
       " ('saturday april 28 1990', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('08 jul 2008', '2008-07-08'),\n",
       " ('8 sep 1999', '1999-09-08'),\n",
       " ('thursday january 1 1981', '1981-01-01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '/': 2,\n",
       " '0': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '5': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '8': 11,\n",
       " '9': 12,\n",
       " 'a': 13,\n",
       " 'b': 14,\n",
       " 'c': 15,\n",
       " 'd': 16,\n",
       " 'e': 17,\n",
       " 'f': 18,\n",
       " 'g': 19,\n",
       " 'h': 20,\n",
       " 'i': 21,\n",
       " 'j': 22,\n",
       " 'l': 23,\n",
       " 'm': 24,\n",
       " 'n': 25,\n",
       " 'o': 26,\n",
       " 'p': 27,\n",
       " 'r': 28,\n",
       " 's': 29,\n",
       " 't': 30,\n",
       " 'u': 31,\n",
       " 'v': 32,\n",
       " 'w': 33,\n",
       " 'y': 34,\n",
       " '<unk>': 35,\n",
       " '<pad>': 36}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-': 0,\n",
       " '0': 1,\n",
       " '1': 2,\n",
       " '2': 3,\n",
       " '3': 4,\n",
       " '4': 5,\n",
       " '5': 6,\n",
       " '6': 7,\n",
       " '7': 8,\n",
       " '8': 9,\n",
       " '9': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '-',\n",
       " 1: '0',\n",
       " 2: '1',\n",
       " 3: '2',\n",
       " 4: '3',\n",
       " 5: '4',\n",
       " 6: '5',\n",
       " 7: '6',\n",
       " 8: '7',\n",
       " 9: '8',\n",
       " 10: '9'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (10000, 30, 37)\n",
      "Shape of Y_train: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = preprocess_data(dataset, human_vocab, machine_vocab)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of Y_train:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(params):\n",
    "    dims = params['dims']\n",
    "    X_train = params['X_train']\n",
    "    Y_train = params['Y_train']\n",
    "    epochs = params['epochs']\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    f1 = tf.keras.layers.Conv1D(filters=48, \n",
    "                                kernel_size=3, \n",
    "                                strides=3,\n",
    "                                padding='valid',\n",
    "                                activation='relu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros')\n",
    "    f2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64,\n",
    "                                                            activation='tanh',\n",
    "                                                            recurrent_activation='sigmoid',\n",
    "                                                            kernel_initializer='glorot_uniform',\n",
    "                                                            bias_initializer='zeros',\n",
    "                                                            recurrent_initializer='zeros',\n",
    "                                                            return_sequences=True,\n",
    "                                                            return_state = False), \n",
    "                                       merge_mode='concat')\n",
    "    f3 = tf.keras.layers.Dense(units=dims[1][1], \n",
    "                               activation='softmax', \n",
    "                               kernel_initializer='glorot_uniform', \n",
    "                               bias_initializer='zeros')\n",
    "    x = tf.keras.Input(shape=dims[0])\n",
    "    a1 = f1(x)\n",
    "    a2 = f2(a1)\n",
    "    y = f3(a2)\n",
    "    model = tf.keras.Model(x, y)    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, \n",
    "                                         beta_1=0.9, \n",
    "                                         beta_2=0.999, \n",
    "                                         epsilon=1e-07)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['categorical_accuracy'], optimizer=optimizer)\n",
    "    model.summary()\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30, 37)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 10, 48)            5376      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 128)           57856     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10, 11)            1419      \n",
      "=================================================================\n",
      "Total params: 64,651\n",
      "Trainable params: 64,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 3s 345us/sample - loss: 1.3601 - categorical_accuracy: 0.4999\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.7895 - categorical_accuracy: 0.6849\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5909 - categorical_accuracy: 0.7639\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.4627 - categorical_accuracy: 0.8190\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.3482 - categorical_accuracy: 0.8729\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.2185 - categorical_accuracy: 0.9278\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.1358 - categorical_accuracy: 0.9584\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 0.0857 - categorical_accuracy: 0.9758\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.0609 - categorical_accuracy: 0.9836\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.0452 - categorical_accuracy: 0.9887\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s 147us/sample - loss: 0.0331 - categorical_accuracy: 0.9923\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.0230 - categorical_accuracy: 0.9952\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.0186 - categorical_accuracy: 0.9961\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 0.0189 - categorical_accuracy: 0.9958\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.0409 - categorical_accuracy: 0.9883\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 0.0136 - categorical_accuracy: 0.9973\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 0.0086 - categorical_accuracy: 0.9985\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 2s 173us/sample - loss: 0.0084 - categorical_accuracy: 0.9983\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 0.0058 - categorical_accuracy: 0.9991\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 0.0046 - categorical_accuracy: 0.9993\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.0119 - categorical_accuracy: 0.9970\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 0.0178 - categorical_accuracy: 0.9952\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 0.0102 - categorical_accuracy: 0.9976\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.0040 - categorical_accuracy: 0.9993\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s 150us/sample - loss: 0.0025 - categorical_accuracy: 0.9997\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 0.0016 - categorical_accuracy: 0.9998\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 0.0013 - categorical_accuracy: 0.9998\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s 150us/sample - loss: 0.0010 - categorical_accuracy: 0.9999\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s 150us/sample - loss: 9.5242e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 8.7252e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 7.1159e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 7.1171e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 6.6566e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 6.6528e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 5.6754e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 5.0243e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.0896 - categorical_accuracy: 0.9758\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 0.0154 - categorical_accuracy: 0.9961\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 0.0089 - categorical_accuracy: 0.9978\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.0043 - categorical_accuracy: 0.9991\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.0084 - categorical_accuracy: 0.9979\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 0.0045 - categorical_accuracy: 0.9989\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 0.0022 - categorical_accuracy: 0.9997\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 0.0010 - categorical_accuracy: 0.9999\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 7.5098e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 5.8908e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 5.8028e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 4.4317e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 4.5532e-04 - categorical_accuracy: 0.9999\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 3.6262e-04 - categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = Model({'dims': [X_train.shape[1:], Y_train.shape[1:]], \n",
    "               'X_train': X_train,\n",
    "               'Y_train': Y_train,\n",
    "               'epochs': 50, \n",
    "               'learning_rate': 0.005,\n",
    "               'batch_size': 64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Date: 3 May 1979 - Predicted Date: 1979-05-03\n",
      "Original Date: 5 April 09 - Predicted Date: 2019-04-05\n",
      "Original Date: 21th of August 2016 - Predicted Date: 2016-09-10\n",
      "Original Date: Tue 10 Jul 2007 - Predicted Date: 2007-08-00\n",
      "Original Date: Saturday May 9 2018 - Predicted Date: 2018-05-09\n",
      "Original Date: March 3 2001 - Predicted Date: 2001-03-03\n",
      "Original Date: March 3rd 2001 - Predicted Date: 2001-03-03\n",
      "Original Date: 1 November 2011 - Predicted Date: 2011-11-01\n",
      "Original Date: September 21st 2020 - Predicted Date: 2020-09-21\n",
      "Original Date: 20/06/21 - Predicted Date: 2021-12-20\n"
     ]
    }
   ],
   "source": [
    "examples = ['3 May 1979', \n",
    "            '5 April 09', \n",
    "            '21th of August 2016', \n",
    "            'Tue 10 Jul 2007', \n",
    "            'Saturday May 9 2018', \n",
    "            'March 3 2001', \n",
    "            'March 3rd 2001', \n",
    "            '1 November 2011',\n",
    "            'September 21st 2020',\n",
    "            '20/06/21']\n",
    "\n",
    "X_examples = np.zeros(shape=(len(examples), 30, len(human_vocab)), dtype='float32')\n",
    "for i, example in enumerate(examples):\n",
    "    sequence = string_to_int(example, 30, human_vocab)\n",
    "    for j, n in enumerate(sequence):\n",
    "        X_examples[i,j,n] = 1.0\n",
    "        \n",
    "Y_examples = model.predict(X_examples)\n",
    "prediction = np.argmax(Y_examples, axis = -1)\n",
    "\n",
    "for i in range(len(examples)):\n",
    "    date = ''\n",
    "    for j in prediction[i,:]:\n",
    "        date = date + inv_machine_vocab[int(j)]\n",
    "    print(\"Original Date: {} - Predicted Date: {}\".format(examples[i], date))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
